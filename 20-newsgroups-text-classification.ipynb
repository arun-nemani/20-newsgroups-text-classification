{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of project\n",
    "- Author: Arun Nemani\n",
    "- Effectively extract features from all 20 categories of the 20-newsgroups dataset\n",
    "- Train and fit a classification model to predict text inputs based on the extracted features\n",
    "- Report the accuracies for each classification models\n",
    "\n",
    "### Cloned repos as baseline\n",
    "- https://github.com/stefansavev/demos/blob/master/text-categorization/20ng/20ng.py\n",
    "- https://nlpforhackers.io/text-classification/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and load dataset\n",
    "\n",
    "First we split the dataset into a training, development, and testing set.\n",
    "The purpose of splitting up the dataset in this manner is to ensure we do not bias or overfit our model by iteratively refining our model on the train and test sets. Instead, we fine tune our model on the train and development set, and invoke the test only once as a final input on the tuned model.\n",
    "\n",
    "Thus, the entire dataset is split into the training set (70%), development set (15%), and test set (15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/arunnemani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "Data loaded\n",
      "\n",
      "Training data documents: 13192\n",
      "Development data documents: 2827\n",
      "Test data documents: 2827\n",
      "\n",
      "Total Newsgroups : ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Initializations\n",
    "nltk.download('punkt')\n",
    "categories = None #Indicating all 20 categories are included in dataset\n",
    "remove = ('headers', 'footers', 'quotes') #Required to remove false features that result in overfitting\n",
    "RANDOM_STATE = 35\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "newsdata = fetch_20newsgroups(subset='all')\n",
    "X_train, X_intermediate, Y_train, Y_intermediate = train_test_split(newsdata.data, newsdata.target, test_size=0.30, random_state=RANDOM_STATE)\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_intermediate, Y_intermediate, test_size=0.50, random_state=RANDOM_STATE)\n",
    "print('Data loaded')\n",
    "print()\n",
    "print('Training data documents:', len(X_train))\n",
    "print('Development data documents:', len(X_dev))\n",
    "print('Test data documents:', len(X_test))\n",
    "print()\n",
    "print('Total Newsgroups :', newsdata.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor 1\n",
    "\n",
    "We start by implementing a simple CountVectorizer to extract features without any preprocessing or parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE1 vocabulary size is 142559 in 13192 documents\n"
     ]
    }
   ],
   "source": [
    "FE1 = CountVectorizer(analyzer= 'word')\n",
    "Vocab = FE1.fit_transform(X_train)\n",
    "print('FE1 vocabulary size is {} in {} documents'.format(Vocab.shape[1], Vocab.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor 2\n",
    "\n",
    "FE1 shows a total vocabulary size of 142559 unique features (words) in 13192 documents.\n",
    "However, there are lots of words in our feature extraction that SHOULD not be considered features (articles, punctuations, etc). This is where we need to incorporate three text preprocessing schemes.\n",
    "- Stopwords: This technique allows the systematic removal of english words that are not unique to a feature or document ('the', 'and', 'is', '.', '?', etc)\n",
    "- Stemming: This technique truncates words into their respective unique root stems. Ex (Flying, flown, flyer, all have the same root)\n",
    "- Tokenization: This process systematically breaks up a corpus or string line into uniquely different words or sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stem_tokenize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE2 vocabulary size is 181542 in 13192 documents\n"
     ]
    }
   ],
   "source": [
    "FE2 = CountVectorizer(analyzer= 'word', tokenizer=Stem_tokenize,\n",
    "                                stop_words=stopwords.words('english') + list(string.punctuation))\n",
    "Vocab = FE2.fit_transform(X_train)\n",
    "print('FE2 vocabulary size is {} in {} documents'.format(Vocab.shape[1], Vocab.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor 3\n",
    "\n",
    "FE2 shows a total vocabulary size of 181542 unique features (words) in 13192 documents.\n",
    "Note that the vocabulary feature size increased from FE2 to FE3, however, FE3 is significantly more robust in classifying unique words due to our preprocessing (accuracy / sanity checks to follow).\n",
    "\n",
    "Now that we have incorporated tokenization, stopwords (with punctuations), and stemming, we can fine tune our feature extraction method. We will remove all accents in our corpus and add bi-grams to our model to ensure that multi-word features are also being classified together. For ex. word such as \"24 bit\" or \"image processing\" are considered one feature and is a bi-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE3 vocabulary size is 1392137 in 13192 documents\n"
     ]
    }
   ],
   "source": [
    "FE3 = CountVectorizer(analyzer= 'word', tokenizer=Stem_tokenize,\n",
    "                                stop_words=stopwords.words('english') + list(string.punctuation),\n",
    "                                lowercase=True, strip_accents='ascii', ngram_range=(1,2))\n",
    "Vocab = FE3.fit_transform(X_train)\n",
    "print('FE3 vocabulary size is {} in {} documents'.format(Vocab.shape[1], Vocab.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor 4\n",
    "\n",
    "FE3 shows a total vocabulary size of 1392137 unique features (words) in 13192 documents.\n",
    "\n",
    "Up till now, we have used a standard CountVectorizer that counts the number of unique features within a dataset. This can be problematic as the frequency of these features are not accounted for. This presents an overfitting challenge particularly for linear approaches.\n",
    "\n",
    "Thus we utilize another feature extractor (TfidfVectorizer) that accounts for the frequency of uniquely identified features in our model. All of the tuning parameters can still be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE4 vocabulary size is 1392137 in 13192 documents\n"
     ]
    }
   ],
   "source": [
    "FE4 = TfidfVectorizer(analyzer= 'word', tokenizer=Stem_tokenize,\n",
    "                                stop_words=stopwords.words('english') + list(string.punctuation),\n",
    "                                lowercase=True, strip_accents='ascii', ngram_range=(1,2))\n",
    "Vocab = FE4.fit_transform(X_train)\n",
    "print('FE4 vocabulary size is {} in {} documents'.format(Vocab.shape[1], Vocab.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor 5\n",
    "\n",
    "Now that we implemented a more robust feature extractor, we need a way to deal with very unique features that may overfit our models. These include features that may be typos, slang, or words used by a very small subset of documents. It is expected that our model accuracy will reduce, however, this approach will prevent overfitting.\n",
    "\n",
    "This is achieved by setting the max_df and min_df parameters as below:\n",
    "- min_df = 5: Remove features that occur in 5 or less documents\n",
    "- max_df = 0.75: Remove features that appear in more than 75% of all documents. These may include common formatting chars or post script symbols part of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE5 vocabulary size is 85540 in 13192 documents\n"
     ]
    }
   ],
   "source": [
    "FE5 = TfidfVectorizer(analyzer= 'word', tokenizer=Stem_tokenize,\n",
    "                                stop_words=stopwords.words('english') + list(string.punctuation),\n",
    "                                lowercase=True, strip_accents='ascii', ngram_range=(1,2),\n",
    "                                min_df=5, max_df= 0.75)\n",
    "Vocab = FE5.fit_transform(X_train)\n",
    "print('FE5 vocabulary size is {} in {} documents'.format(Vocab.shape[1], Vocab.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalized Feature Extractor\n",
    "\n",
    "Note that we have SIGNIFICANTLY reduced the number of vocabulary features in our vectorizer from 1392137 to 85540.\n",
    "This is primarily due to the min_df parameter since changing the max_df does not impact the feature size significantly.\n",
    "Basically, our previous vectorizers were overfitting features that were very sparse in the dataset.\n",
    "\n",
    "FE5 will be our finalized feature extractor for this project.\n",
    "Next we explore classification schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Classification\n",
    "\n",
    "First we need to understand the dataset before selection a classification model.\n",
    "The matrix output of the feature extraction methods are very sparse with a small set of non-zero values. \n",
    "\n",
    "It is important to note that we will fine tune our classification models on the dev set USING the feature extraction model created on the training set.\n",
    "\n",
    "Thus we will try Multinomial Naive-Bayes, regularized Logistic regression, and Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE5 training set vocabulary size is 85540 in 13192 documents\n",
      "FE5 dev set vocabulary size is 85540 in 2827 documents\n"
     ]
    }
   ],
   "source": [
    "FE5 = TfidfVectorizer(analyzer= 'word', tokenizer= Stem_tokenize,\n",
    "                                stop_words=stopwords.words('english') + list(string.punctuation),\n",
    "                                lowercase=True, strip_accents='ascii', ngram_range=(1,2),\n",
    "                                min_df=5, max_df= 0.75)\n",
    "Vocab_train = FE5.fit_transform(X_train)\n",
    "Vocab_dev = FE5.transform(X_dev)\n",
    "print('FE5 training set vocabulary size is {} in {} documents'.format(Vocab_train.shape[1], Vocab_train.shape[0]))\n",
    "print('FE5 dev set vocabulary size is {} in {} documents'.format(Vocab_dev.shape[1], Vocab_dev.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB optimal alpha: {'alpha': 0.01}\n",
      "Logistic Regression optimal C: {'C': 5.0}\n",
      "Stochastic Gradient Descent optimal alpha: {'alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "classifier_nb = MultinomialNB()\n",
    "params = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0]}\n",
    "grid_classifier_nb = GridSearchCV(classifier_nb, params, scoring = 'accuracy')\n",
    "grid_classifier_nb.fit(Vocab_train, Y_train)\n",
    "pred = grid_classifier_nb.predict(Vocab_dev)\n",
    "print(\"Multinomial NB optimal alpha: {}\".format(grid_classifier_nb.best_params_))\n",
    "\n",
    "classifier_lreg = LogisticRegression(penalty = 'l2', solver='sag', random_state=RANDOM_STATE, n_jobs=-1)\n",
    "params = {'C':[0.0001, 0.001, 0.01, 0.1, 0.25, 0.5, 1.0, 2.0, 5.0]}\n",
    "grid_classifier_lreg = GridSearchCV(classifier_lreg, params, scoring = 'accuracy')\n",
    "grid_classifier_lreg.fit(Vocab_train, Y_train)\n",
    "pred = grid_classifier_lreg.predict(Vocab_dev)\n",
    "print(\"Logistic Regression optimal C: {}\".format(grid_classifier_lreg.best_params_))\n",
    "\n",
    "classifier_SGD = SGDClassifier(tol=0.0001, penalty=\"l2\", random_state=RANDOM_STATE, n_jobs=-1)\n",
    "params = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0]}\n",
    "grid_classifier_SGD = GridSearchCV(classifier_SGD, params, scoring = 'accuracy')\n",
    "grid_classifier_SGD.fit(Vocab_train, Y_train)\n",
    "pred = grid_classifier_SGD.predict(Vocab_dev)\n",
    "print(\"Stochastic Gradient Descent optimal alpha: {}\".format(grid_classifier_SGD.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal classification model\n",
    "\n",
    "Now that we have identified the optimal parameters for each model. We will calculate the final accuracies on the test set and select the final classification model for this project.\n",
    "\n",
    "Note that we predefine the regularization parameters for SGD and logistic regression classifiers.\n",
    "\n",
    "The idea of regularization is to avoid learning very large weights, which are likely to fit the training data but do not generalize well. L2 regularization adds a penalty to the sum of the squared weights whereas L1 regularization computes add the penalty via the sum of the absolute values of the weights. The result is that L2 regularization makes all the weights relatively small, and L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "In this particular application, there are a number of features that are very sparse but unique in identifying newsgroups.\n",
    "Thus, only \"L2\" regularization has been selected for all classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB classifier accuracy: 0.9073\n",
      "Logistic regression classifier accuracy: 0.9059\n",
      "Stochastic Gradient Descent classifier accuracy: 0.9165\n"
     ]
    }
   ],
   "source": [
    "Vocab_test = FE5.transform(X_test)\n",
    "classifier_NB = MultinomialNB(alpha=0.01)\n",
    "classifier_NB.fit(Vocab_train, Y_train)\n",
    "pred = classifier_NB.predict(Vocab_test)\n",
    "print(\"NB classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_test, pred),4)))\n",
    "\n",
    "classifier_lreg = LogisticRegression(penalty = 'l2', solver='sag', C=5, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "classifier_lreg.fit(Vocab_train, Y_train)\n",
    "pred = classifier_lreg.predict(Vocab_test)\n",
    "print(\"Logistic regression classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_test, pred),4)))\n",
    "\n",
    "classifier_SGD = SGDClassifier(tol=0.0001, penalty=\"l2\", alpha=0.0001, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "classifier_SGD.fit(Vocab_train, Y_train)\n",
    "pred = classifier_SGD.predict(Vocab_test)\n",
    "print(\"Stochastic Gradient Descent classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_test, pred),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalized predictive model\n",
    "\n",
    "Based on our preprocessing, parameter tuning, and model selection, a stochastic gradient descent classifier can accurately predict an input corpus into one of the 20 newsgroups. However, multinomial NB has marginally lower accuracy but perfoms significantly much faster than the other classifiers. \n",
    "\n",
    "Below we apply some sanity checks to see our prediction work in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 newsgroups: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Predicted newsgroup: rec.motorcycles\n",
      "Predicted newsgroup: comp.windows.x\n",
      "Predicted newsgroup: rec.sport.hockey\n",
      "Predicted newsgroup: misc.forsale\n",
      "Predicted newsgroup: talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "def predictNewsGroup(text, clf):\n",
    "    Vocab_test = FE5.transform([text])\n",
    "    targets = newsdata.target_names\n",
    "    idx = clf.predict(Vocab_test)\n",
    "    print(\"Predicted newsgroup: {}\".format(targets[int(idx)]))\n",
    "    return\n",
    "\n",
    "print(\"20 newsgroups: {}\".format(newsdata.target_names))\n",
    "print()\n",
    "predictNewsGroup(\"A Honda CBR is a dope bike\", classifier_SGD)\n",
    "predictNewsGroup(\"NLP scripts are ownage\", classifier_SGD)\n",
    "predictNewsGroup(\"He is #1 player with the highest contract signed for the Minnesota Wild\", classifier_SGD)\n",
    "predictNewsGroup(\"I'll sell my Gamecube for $1000\", classifier_SGD)\n",
    "predictNewsGroup(\"Homs is really unstable right now\", classifier_SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing remarks\n",
    "\n",
    "While we developed a fairly accurate classifier to predict the news category based on an input corpus, there are several approaches for further model refinement outlined below:\n",
    "\n",
    "- \n",
    "### Alternative classifiers\n",
    "I specificaly chose Stochastic Gradient Descent classifier since this approach assumes conditional independence of the features (words). There are several approaches other approaches (such, support vector machines, ensemble methods, Random Forest, etc) that can also be tuned and applied. However, for the purposes of this project, the focus is on the feature extraction methods.\n",
    "- \n",
    "### Optimization\n",
    "Several oppurtunities for optimization. Namely, the nasty for loop for stemming. There is limited stemming support in feature extractors thus I had to resort to making my own implementation.\n",
    "- \n",
    "### Feature extraction\n",
    "Lots of opportunities to fine tune feature extraction that include tuning of max_df and min_df parameters, custom corpus filtering functions to remove web scrapping metadata, etc.\n",
    "\n",
    "- \n",
    "### Alternative model performance metrics\n",
    "We are currently reporting model performane using accuracy, but this may be improved by utilizing other metrics such as f-score, confusion matrices, CV error, and the BLEU score (specific to NLP problems)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
