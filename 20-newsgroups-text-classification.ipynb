{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of project\n",
    "\n",
    "- Utilize all 20 categories that are a part of the 20-newsgroups dataset and classify extracted features\n",
    "- Report the accuracies for each classification models\n",
    "\n",
    "# Cloned repos as baseline\n",
    "- https://github.com/stefansavev/demos/blob/master/text-categorization/20ng/20ng.py\n",
    "- https://nlpforhackers.io/text-classification/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/arunnemani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "Data loaded\n",
      "Accuracy: 0.8550509337860781\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Initializations\n",
    "nltk.download('punkt')\n",
    "categories = None #Indicating all 20 categories are included in dataset\n",
    "remove = ('headers', 'footers', 'quotes') #Required to false features that may be included, resulting in overfitting\n",
    "RANDOM_STATE = 35\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "newsdata = fetch_20newsgroups(subset='all')\n",
    "print('Data loaded')\n",
    "\n",
    "def trainClassifier(clf, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)\n",
    "    clf.fit(X_train, y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy: {}\".format(metrics.accuracy_score(y_test, Y_pred)))\n",
    "    return\n",
    "\n",
    "Model1 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())])\n",
    "\n",
    "trainClassifier(Model1, newsdata.data, newsdata.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model1 \n",
    "\n",
    "We just implemented a simple Multinomial Naive-Bayes classification model utilizing the CountVectorizer to extract features. Now, we will interate step by step to improve our model robustness and accuracy.\n",
    "\n",
    "First, we need to implement three main features for this project\n",
    "- Stopwords: This is to prevent common articles, punctuation, etc in our traning model.\n",
    "- Stemming: This is to prevent multiple occurances of the same word within our model (eg. fly and flying are the same)\n",
    "- Tokenizer: This is to break up entire sentences or lines into meaningful words, phrases, or \"tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stem_tokenize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8404074702886248\n"
     ]
    }
   ],
   "source": [
    "Model2 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=Stem_tokenize,\n",
    "                             stop_words=stopwords.words('english') + list(string.punctuation))),\n",
    "    ('classifier', MultinomialNB())])\n",
    "\n",
    "trainClassifier(Model2, newsdata.data, newsdata.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model2\n",
    "\n",
    "Now that we have incorporated tokenization, stopwords (with punctuations), and stemming, we can fine tune our feature extraction method. Note that the acurracy dropped between Model1 and Model2, however, it is significantly more robust in classifying key words towards newsgroups (sanity checks to follow).\n",
    "\n",
    "We will add bi-grams to our model to ensure that multi word features are also being classified together.\n",
    "For ex. \"24 bit\" or \"image processing\" are considered one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8711799660441426\n"
     ]
    }
   ],
   "source": [
    "Model3 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(tokenizer=Stem_tokenize,\n",
    "                             stop_words=stopwords.words('english') + list(string.punctuation),\n",
    "                                  lowercase=True, strip_accents='ascii', ngram_range=(1,2))),\n",
    "    ('classifier', MultinomialNB())])\n",
    "\n",
    "trainClassifier(Model3, newsdata.data, newsdata.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model3\n",
    "\n",
    "Up till now, we have used a standard CountVectorizer that counts the number of unique features within a dataset. This can be problematic as the frequency of these features are not accounted for. This presents and overfitting challenge particularly for linear approaches.\n",
    "\n",
    "Thus we utilize another feature extractor (TfidfVectorizer) that accounts for the frequency of uniquely identified features in our model. All of the tuning parameters can still be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8739388794567062\n"
     ]
    }
   ],
   "source": [
    "Model4 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=Stem_tokenize,\n",
    "                             stop_words=stopwords.words('english') + list(string.punctuation),\n",
    "                                  lowercase=True, strip_accents='ascii', ngram_range=(1,2))),\n",
    "    ('classifier', MultinomialNB())])\n",
    "\n",
    "trainClassifier(Model4, newsdata.data, newsdata.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model4\n",
    "\n",
    "With the incorporation of TfidfVectorizer, we see a slight increase in accuracy.\n",
    "Further optimization\n",
    "- max_df and min_df: Further tuning these parameters can weight towards features that are not considered outliers.\n",
    "\n",
    "Now, we can fine tune the classifcation algorithm (MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9320882852292021\n"
     ]
    }
   ],
   "source": [
    "Model5 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=Stem_tokenize,\n",
    "                             stop_words=stopwords.words('english') + list(string.punctuation),\n",
    "                                  lowercase=True, strip_accents='ascii', ngram_range=(1,2))),\n",
    "    ('classifier', MultinomialNB(alpha=.01))])\n",
    "\n",
    "trainClassifier(Model5, newsdata.data, newsdata.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model5\n",
    "\n",
    "Tuning the alpha parameter (this is simply a Laplace smoothing coefficent), we increased our model accuracy by ~6%!\n",
    "Next we can try Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arunnemani/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8909168081494058\n"
     ]
    }
   ],
   "source": [
    "Model6 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=Stem_tokenize,\n",
    "                             stop_words=stopwords.words('english') + list(string.punctuation),\n",
    "                                  lowercase=True, strip_accents='ascii', ngram_range=(1,2))),\n",
    "    ('classifier', SGDClassifier(alpha=.0001, n_iter=100, penalty=\"elasticnet\"))])\n",
    "\n",
    "trainClassifier(Model6, newsdata.data, newsdata.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model6\n",
    "\n",
    "SGD approach yielded in a lower accuracy score, however, this can be improved through further parameter tuning.\n",
    "\n",
    "We will select Model5 as our finalized model and test out some text inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sci.med'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictNewsGroup(text, pipeline):\n",
    "  targets = newsdata.target_names\n",
    "  idx = pipeline.predict([text])[0]\n",
    "  return targets[idx]\n",
    "targets = newsdata.target_names\n",
    "print(targets)\n",
    "\n",
    "predictNewsGroup(\"A Honda CBR is a meh bike\", Model5)\n",
    "predictNewsGroup(\"This NLP script is very godlike\", Model5)\n",
    "predictNewsGroup(\"Why would\", Model5)\n",
    "predictNewsGroup(\"This NLP script is very godlike\", Model5)\n",
    "predictNewsGroup(\"This NLP script is very godlike\", Model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
