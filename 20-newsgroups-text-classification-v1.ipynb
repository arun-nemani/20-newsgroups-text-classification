{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of project\n",
    "- Author: Arun Nemani\n",
    "- Effectively extract features from N categories of the 20-newsgroups dataset\n",
    "- Train and fit a classification model to predict text inputs based on the extracted features\n",
    "- Report the accuracies for each classification models\n",
    "\n",
    "### Cloned repos as baseline\n",
    "- https://github.com/stefansavev/demos/blob/master/text-categorization/20ng/20ng.py\n",
    "- https://nlpforhackers.io/text-classification/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and load dataset\n",
    "\n",
    "First we split the dataset into a training, development, and testing set.\n",
    "The purpose of splitting up the dataset in this manner is to ensure we do not bias or overfit our model by iteratively refining our model on the train and test sets. Instead, we fine tune our model on the train and development set, and invoke the test only once as a final input on the tuned model.\n",
    "\n",
    "Thus, the entire dataset is split into the training set (70%), development set (15%), and test set (15%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/m1u6026/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/m1u6026/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "Data loaded\n",
      "\n",
      "Training data documents: 13192\n",
      "Development data documents: 2827\n",
      "Test data documents: 2827\n",
      "\n",
      "Total Newsgroups : ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Initializations\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "categ = ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', \n",
    "              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', \n",
    "              'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', \n",
    "              'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', \n",
    "              'sci.space', 'soc.religion.christian', 'talk.politics.guns', \n",
    "              'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'] #None: Indicating all 20 categories are included in dataset\n",
    "remove = ('headers', 'footers', 'quotes') #Required to remove false features that result in overfitting\n",
    "RANDOM_STATE = 35\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "newsdata = fetch_20newsgroups(subset='all', categories=categ)\n",
    "X_train, X_intermediate, Y_train, Y_intermediate = train_test_split(newsdata.data, newsdata.target, test_size=0.30, random_state=RANDOM_STATE)\n",
    "X_dev, X_test, Y_dev, Y_test = train_test_split(X_intermediate, Y_intermediate, test_size=0.50, random_state=RANDOM_STATE)\n",
    "print('Data loaded')\n",
    "print()\n",
    "print('Training data documents:', len(X_train))\n",
    "print('Development data documents:', len(X_dev))\n",
    "print('Test data documents:', len(X_test))\n",
    "print()\n",
    "print('Total Newsgroups :', newsdata.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stem_tokenize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalized Feature Extractor\n",
    "\n",
    "Note that we have SIGNIFICANTLY reduced the number of vocabulary features in our vectorizer from 1392137 to 85540.\n",
    "This is primarily due to the min_df parameter since changing the max_df does not impact the feature size significantly.\n",
    "Basically, our previous vectorizers were overfitting features that were VERY sparse in the dataset.\n",
    "\n",
    "FE will be our finalized feature extractor for this project.\n",
    "Next we explore classification schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE training set vocabulary size is 85540 in 13192 documents\n",
      "FE dev set vocabulary size is 85540 in 2827 documents\n"
     ]
    }
   ],
   "source": [
    "FE = TfidfVectorizer(analyzer= 'word', tokenizer= Stem_tokenize,\n",
    "                                stop_words=stopwords.words('english') + list(string.punctuation),\n",
    "                                lowercase=True, strip_accents='ascii', ngram_range=(1,2),\n",
    "                                min_df=5, max_df= 0.75)\n",
    "Vocab_train = FE.fit_transform(X_train)\n",
    "Vocab_dev = FE.transform(X_dev)\n",
    "print('FE training set vocabulary size is {} in {} documents'.format(Vocab_train.shape[1], Vocab_train.shape[0]))\n",
    "print('FE dev set vocabulary size is {} in {} documents'.format(Vocab_dev.shape[1], Vocab_dev.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Classification\n",
    "\n",
    "First we need to understand the dataset before selection a classification model.\n",
    "The matrix output of the feature extraction methods are very sparse with a small set of non-zero values. \n",
    "\n",
    "It is important to note that we will fine tune our classification models on the dev set USING the feature extraction model created on the training set.\n",
    "\n",
    "Thus we will try Multinomial Naive-Bayes, regularized Logistic regression, and Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-search for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB optimal alpha: {'alpha': 0.01}\n",
      "Logistic Regression optimal C: {'C': 5.0}\n",
      "Stochastic Gradient Descent optimal alpha: {'alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "classifier_nb = MultinomialNB()\n",
    "params = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0]}\n",
    "grid_classifier_nb = GridSearchCV(classifier_nb, params, scoring = 'accuracy')\n",
    "grid_classifier_nb.fit(Vocab_train, Y_train)\n",
    "pred = grid_classifier_nb.predict(Vocab_dev)\n",
    "print(\"Multinomial NB optimal alpha: {}\".format(grid_classifier_nb.best_params_))\n",
    "\n",
    "classifier_lreg = LogisticRegression(penalty = 'l2', solver='sag', random_state=RANDOM_STATE, n_jobs=-1)\n",
    "params = {'C':[0.0001, 0.001, 0.01, 0.1, 0.25, 0.5, 1.0, 2.0, 5.0]}\n",
    "grid_classifier_lreg = GridSearchCV(classifier_lreg, params, scoring = 'accuracy')\n",
    "grid_classifier_lreg.fit(Vocab_train, Y_train)\n",
    "pred = grid_classifier_lreg.predict(Vocab_dev)\n",
    "print(\"Logistic Regression optimal C: {}\".format(grid_classifier_lreg.best_params_))\n",
    "\n",
    "classifier_SGD = SGDClassifier(tol=0.0001, penalty=\"l2\", random_state=RANDOM_STATE, n_jobs=-1)\n",
    "params = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0]}\n",
    "grid_classifier_SGD = GridSearchCV(classifier_SGD, params, scoring = 'accuracy')\n",
    "grid_classifier_SGD.fit(Vocab_train, Y_train)\n",
    "pred = grid_classifier_SGD.predict(Vocab_dev)\n",
    "print(\"Stochastic Gradient Descent optimal alpha: {}\".format(grid_classifier_SGD.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal classification model\n",
    "\n",
    "Now that we have identified the optimal parameters for each model, we will calculate the final accuracies on the test set and select the final classification model for this project.\n",
    "\n",
    "Note that we predefine the regularization parameters for SGD and logistic regression classifiers.\n",
    "\n",
    "The idea of regularization is to avoid learning very large weights, which are likely to fit the training data but do not generalize well. L2 regularization adds a penalty to the sum of the squared weights whereas L1 regularization computes add the penalty via the sum of the absolute values of the weights. The result is that L2 regularization makes all the weights relatively small, and L1 regularization drives lots of the weights to 0, effectively removing unimportant features.\n",
    "\n",
    "In this particular application, there are a number of features that are very sparse but unique in identifying newsgroups.\n",
    "Thus, only \"L2\" regularization has been selected for all classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB classifier accuracy: 0.9073\n",
      "Logistic regression classifier accuracy: 0.9059\n",
      "Stochastic Gradient Descent classifier accuracy: 0.9165\n"
     ]
    }
   ],
   "source": [
    "Vocab_test = FE.transform(X_test)\n",
    "classifier_NB = MultinomialNB(alpha=0.01)\n",
    "classifier_NB.fit(Vocab_train, Y_train)\n",
    "pred = classifier_NB.predict(Vocab_test)\n",
    "print(\"NB classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_test, pred),4)))\n",
    "\n",
    "classifier_lreg = LogisticRegression(penalty = 'l2', solver='sag', C=5, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "classifier_lreg.fit(Vocab_train, Y_train)\n",
    "pred = classifier_lreg.predict(Vocab_test)\n",
    "print(\"Logistic regression classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_test, pred),4)))\n",
    "\n",
    "classifier_SGD = SGDClassifier(tol=0.0001, penalty=\"l2\", alpha=0.0001, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "classifier_SGD.fit(Vocab_train, Y_train)\n",
    "pred = classifier_SGD.predict(Vocab_test)\n",
    "print(\"Stochastic Gradient Descent classifier accuracy: {}\".format(round(metrics.accuracy_score(Y_test, pred),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalized predictive model\n",
    "\n",
    "Multinomial NB classifier accuracy on final test set: 0.9073.\n",
    "\n",
    "Logistic regression accuracy on final test set: 0.9059.\n",
    "\n",
    "Stochastic gradient descent accuracy on final test set: 0.9165.\n",
    "\n",
    "Based on our preprocessing, parameter tuning, and model selection, a stochastic gradient descent classifier can accurately predict an input corpus into one of the 20 newsgroups. However, multinomial NB has marginally lower accuracy but perfoms significantly much faster than the other classifiers on the new test set. \n",
    "\n",
    "Below we apply some sanity checks to see our prediction work in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroup categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Predicted newsgroup: rec.motorcycles\n",
      "Predicted newsgroup: rec.sport.hockey\n",
      "Predicted newsgroup: misc.forsale\n",
      "Predicted newsgroup: talk.politics.mideast\n",
      "Predicted newsgroup: sci.space\n",
      "Predicted newsgroup: rec.autos\n",
      "Predicted newsgroup: sci.med\n"
     ]
    }
   ],
   "source": [
    "def predictNewsGroup(text, clf):\n",
    "    Vocab_test = FE.transform([text])\n",
    "    targets = newsdata.target_names\n",
    "    idx = clf.predict(Vocab_test)\n",
    "    print(\"Predicted newsgroup: {}\".format(targets[int(idx)]))\n",
    "    return\n",
    "\n",
    "print(\"Newsgroup categories: {}\".format(newsdata.target_names))\n",
    "print()\n",
    "predictNewsGroup(\"A Honda CBR is a dope ride\", classifier_SGD)\n",
    "predictNewsGroup(\"He is #1 player with the highest contract signed for the Minnesota Wild\", classifier_SGD)\n",
    "predictNewsGroup(\"I'll only sell with my Gamecube for $1000\", classifier_SGD)\n",
    "predictNewsGroup(\"Homs is really unstable right now. Many refugees are actively leaving the region\", classifier_SGD)\n",
    "predictNewsGroup(\"Interstellar was a really good movie. I'm sure Carl Sagan would've loved it\", classifier_SGD)\n",
    "predictNewsGroup(\"MLops is an important concept for productionalizing machine learning models\", classifier_SGD)\n",
    "predictNewsGroup(\"Donald Trump didn't tell the whole truth about the Russia investigation\", classifier_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
